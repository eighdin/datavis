{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0934aa50",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "* we're doing the whole NLP process today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2f40d",
   "metadata": {},
   "source": [
    "### Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7fe5f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>Secretary of Commerce Charles W. Sawyer said y...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>U.S. stocks inched up last week, overcoming co...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>Ben S. Bernanke cleared a key hurdle Thursday ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>The White House's push to contract out many fe...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>NEW YORK. April 17-Automobile stocks put on th...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text relevance\n",
       "0     NEW YORK -- Yields on most certificates of dep...       yes\n",
       "1     The Wall Street Journal Online</br></br>The Mo...        no\n",
       "2     WASHINGTON -- In an effort to achieve banking ...        no\n",
       "3     The statistics on the enormous costs of employ...        no\n",
       "4     NEW YORK -- Indecision marked the dollar's ton...       yes\n",
       "...                                                 ...       ...\n",
       "7995  Secretary of Commerce Charles W. Sawyer said y...       yes\n",
       "7996  U.S. stocks inched up last week, overcoming co...        no\n",
       "7997  Ben S. Bernanke cleared a key hurdle Thursday ...        no\n",
       "7998  The White House's push to contract out many fe...        no\n",
       "7999  NEW YORK. April 17-Automobile stocks put on th...        no\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "original = pd.read_csv('economic_news.csv', encoding = 'ISO-8859-1')\n",
    "required_data = original[['text', 'relevance']]\n",
    "required_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812dffd",
   "metadata": {},
   "source": [
    "Removing invalid labels from the dataset based on a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "354854cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "no     6571\n",
       "yes    1420\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_data.drop(required_data.loc[required_data['relevance'] == \"not sure\"].index, inplace = True)\n",
    "required_data['relevance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e9394",
   "metadata": {},
   "source": [
    "Largest class should be less than 2x smallest class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be4172",
   "metadata": {},
   "source": [
    "### Text Cleaning and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aeac081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c572fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\t\tFalse\n",
      ",\t\tFalse\n",
      "I\t\tTrue\n",
      "like\t\tFalse\n",
      "3\t\tFalse\n",
      "Nolan\t\tFalse\n",
      "'s\t\tTrue\n",
      ",\t\tFalse\n",
      "two\t\tTrue\n",
      "Cameron\t\tFalse\n",
      "'s\t\tTrue\n",
      "&\t\tFalse\n",
      "12\t\tFalse\n",
      "Spielberg\t\tFalse\n",
      "'s\t\tTrue\n",
      "movies\t\tFalse\n",
      ".\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "sample_doc = nlp(\"Hi, I like 3 Nolan's, two Cameron's & 12 Spielberg's movies.\")\n",
    "for token in sample_doc:\n",
    "    #print(f\"token: {token}\\t\\t| is punct? {token.is_punct}\\t\\t| like num? {token.like_num}\\t\\t| is digit? {token.is_digit}\")\n",
    "    print(f\"{token}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b4bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_preprocess(text):\n",
    "    # Lowercase\n",
    "    outtext = text.lower()\n",
    "    # Remove unwanted/weird characters\n",
    "    outtext = outtext.replace('</br>', ' ')\n",
    "    # Remove punctuation and digits\n",
    "    doc = nlp(outtext)\n",
    "    outtext = [token.text for token in doc if (not token.is_punct) and (not token.like_num)]\n",
    "    # remove stop words\n",
    "    ## SPACY\n",
    "    # text = [token.text for token in doc if (not token.is_stop)]\n",
    "    ## SCIKIT\n",
    "    outtext = [word for word in outtext if word not in ENGLISH_STOP_WORDS]\n",
    "    processed_text = ' '.join(outtext) # join list of words into sentences\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65a819da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"like nolan 's spielberg 's movies awesome\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_preprocess(\"I like Nolan's, and Spielberg's movies. They are 2 awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd2296b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_data.text = required_data.text.apply(lambda row: clean_preprocess(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d0779",
   "metadata": {},
   "source": [
    "### Feature Engineering or Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518923b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "865eed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"dog likes cat\", \"dog eats meat\", \"cat eats meat\", \"dog bites man\", \"dog likes dog\"]\n",
    "vect = CountVectorizer()\n",
    "x_data = vect.fit_transform(required_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a6c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "        ..\n",
       "7995   NaN\n",
       "7996   NaN\n",
       "7997   NaN\n",
       "7998   NaN\n",
       "7999   NaN\n",
       "Name: relevance, Length: 7991, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(required_data['relevance'])\n",
    "required_data['relevance'] = required_data['relevance'].map({'yes':1, 'no':0})\n",
    "y_data = required_data['relevance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25edf1ff",
   "metadata": {},
   "source": [
    "### Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a67af4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e5c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "        ..\n",
       "7995   NaN\n",
       "7996   NaN\n",
       "7997   NaN\n",
       "7998   NaN\n",
       "7999   NaN\n",
       "Name: relevance, Length: 7991, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8a61741",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = MultinomialNB()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:767\u001b[39m, in \u001b[36m_BaseDiscreteNB.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    748\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[32m    749\u001b[39m \n\u001b[32m    750\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    765\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     X, y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m     _, n_features = X.shape\n\u001b[32m    770\u001b[39m     labelbin = LabelBinarizer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/naive_bayes.py:613\u001b[39m, in \u001b[36m_BaseDiscreteNB._check_X_y\u001b[39m\u001b[34m(self, X, y, reset)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    612\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2917\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2918\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     out = X, y\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1331\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m   1314\u001b[39m X = check_array(\n\u001b[32m   1315\u001b[39m     X,\n\u001b[32m   1316\u001b[39m     accept_sparse=accept_sparse,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1328\u001b[39m     input_name=\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1329\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m y = \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1333\u001b[39m check_consistent_length(X, y)\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1353\u001b[39m, in \u001b[36m_check_y\u001b[39m\u001b[34m(y, multi_output, y_numeric, estimator)\u001b[39m\n\u001b[32m   1351\u001b[39m     estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1352\u001b[39m     y = column_or_1d(y, warn=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1354\u001b[39m     _ensure_no_complex_data(y)\n\u001b[32m   1355\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y.dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:133\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Project-Files/class-code/NLP/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:182\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    168\u001b[39m     msg_err += (\n\u001b[32m    169\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dabcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a80a3",
   "metadata": {},
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c33190",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
